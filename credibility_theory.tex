\externaldocument{glm}



\subsection{Использование доверительных моделей в многоуровневых факторах}

\subsubsection{Модель Бюльмана-Штрауба}

	Данная модель рассматривает частный случай нашей модели. А именно, в этой модели присутствует только один фактор, и он является многоуровневым. Здесь будут приведены важные результаты, которые будут перенесены на рассматриваемую нами модель.



	Рассмотрим многоуровневый фактор $F$ с уровнями $\{ 1, 2, ..., J\}$. Будем обозначать наблюдения рассматриваемой случайной величины, как $Y_{jt}$, где $j$ - уровень фактора $F$, $t$ - номер среди таких наблюдений. $\mu$ - среднее по всем наблюдениям:
	

$$
	\mu = E[Y_{jt}]
$$



Для каждого уровня $j \in \{1, ..., J\}$ посчитаем для него среднее взвешенное:

$$
	\overline{Y}_j = \frac{\sum_{t}w_{jt}Y_{jt}}{\sum_{jt}w_{jt}}
$$

Здесь, в отличии от обычного фактора, метод максимального правдоподобия неприменим, т.к. каждый из коэффициентов $e^{\beta_j}$ будет рассчитываться по слишком малому количеству наблюдений.

Для каждого уровня $j$ будем делать разумное предсказание на реальное среднее наблюдаемой случайной величины. Здесь нужно найти некоторый компромисс между $\overline{Y}_j$ и $\mu$. Первый является нестабильным, поскольку посчитан на малых данных. Второй стабильный, но никак не отражает зависимости от уровня $j$.

Будем считать, что каждый уровень подвержен \textit{случайному эффекту} $U_j$. Для мультипликативной модели имеем:
$$
	E[Y_{jt} | U_j] = \mu U_j
$$
,где $E[U_j] = 1$

Для удобства сделаем замену $V_j = \mu U_j$. Тогда имеем:
$$
	E[Y_{jt} | V_j] = V_j
$$
,где $E[V_j] = \mu$. Также необходимо сделать предположение о существовании дисперсии у $V_j$.

Здесь мы будем рассматривать случай распределения Твиди, это будет иметь связь с ОЛМ. Тогда имеем

$$
D[Y_{jt} | V_j] = \frac{\phi V_j^p}{w_{jt}}
$$

Предполагая, что все $V_j$ одинаково распределены, можем обозначить $\sigma^2 = \phi E[V_j^p]$. Тогда получим

$$
E[D[Y_{jt} | V_j]] = \frac{\sigma^2}{w_{jt}}
$$



Выпишем все сделанные нами предположения
\begin{itemize}
	\item Случайные векторы $(Y_{jt}, V_j)$ независимы, $j = 1, ... , J$
	\item $V_j$ одинаково распределены со средним $E[V_j] = \mu > 0$, и дисперсией $D[V_j] = \tau^2$, для некоторого $\tau^2$, $j = 1, ... , J$
	\item $\forall j$ все $Y_{jt}$ при условии $V_j$ независимы со средним $E[Y_{jt} | V_j] = V_j$
\end{itemize}


Задумаемся о том, как наилучшим образом оценить влияние каждого $V_j$.
Назовём \textit{достоверной оценкой} случайного эффекта $V$ линейную функцию $\widehat{V}$ наблюдений $Y$, которая минимизирует среднюю квадратичную ошибку

$$
E[(h(Y) - V) ^ 2]
$$

среди всех линейных функций $h(Y)$.

В работе \cite{Su80} было установлено существование и единственность достоверной оценки.
Следующая теорема показывает, как достоверная оценка может быть найдена.


\textbf{Теорема} (Бюльмана-Штрауба) Достоверная оценка для $V_j$ может быть найдена как

\begin{equation} \label{eq:credib_est}
\widehat{V}_j = z_j \overline{Y}_j + (1 - z_j) \mu
\end{equation}

$$
z_j = \frac{w_j}{w_j + \sigma^2/\tau^2}
$$

$z_j$ называется \textit{коэффициентом доверия}



В \cite{OJ10} приведено доказательство этого факта. 


\subsubsection{Оценка параметров}

Для получения достоверной оценки необходимо оценить параметры дисперсии $\sigma ^ 2$ и $\tau ^ 2$. Возможно также необходимо оценить и $\mu$. Для последнего можно использовать среднее взвешенное.

$$
	\mu = \frac{\sum_{jt}w_{jt}Y_{jt}}{\sum_{jt}w_{jt}}
$$

Параметр $\mu$ может быть и известен. Теперь перейдём к параметрам дисперсии.

Параметр $\sigma^2$ описывает среднеквадратическое отклонение от среднего внутри каждой группы. Обозначим $n_j$ - количество наблюдений внутри уровня $j$. Тогда среднеквадратическое отклонение от среднего внутри уровня $j$ можно оценить как

$$
\widehat{\sigma}^2_j = \frac{1}{n_j - 1} \sum_t w_{jt} (Y_{jt} - \overline{Y}_{j})^2 
$$
комбинируя эти оценки, получаем 
$$
\widehat{\sigma}^2 = \frac{\sum_j(n_j - 1) \widehat{\sigma}^2_j}{\sum_j(n_j - 1)} 
$$

Аналогичным образом, $\tau$ - мера дисперсии между группами и её оценка основывается на сумме 
$$
	\sum_j w_j(\overline{Y}_j - \overline{Y})^2
$$

Следующее выражение даёт несмещённую оценку для $\tau$.

$$
\widehat{\tau}^2 = \frac{\sum_t w_{j} (\overline{Y}_j - \overline{Y}) ^ 2 - (J - 1) \widehat{\sigma}^2}
{w - \sum_j w_j^2 / w} 
$$

В \cite{OJ10} приводится доказательство несмещённости данных оценок, то есть
\begin{itemize}
	\item $E[\widehat{\sigma}^2] = \sigma^2$
	\item $E[\widehat{\tau}^2] = \tau^2$	
\end{itemize}


\subsubsection{Достоверные модели в мультипликативных моделях}

Покажем, как применить полученные результаты в мультипликативных моделях с одним многоуровневым фактором. 

Обозначим $R$ - количество обычных факторов. Эти факторы в совокупности образуют множество тарифных ячеек. Рассмотим тарифную ячейку $i$. Для данной тарифной ячейки обычные факторы образуют коффициенты $\gamma_1^i, \gamma_2^i, ... , \gamma_R^i$. Здесь мы переобозначили $e^{\beta_r^i} = \gamma_r^i$, использовавшееся ранее. Обозначения для многоуовневого фактора мы сохраним из предыдущего раздела. Тогда мы получаем следующую модель:
$$
E[Y_{ijt} | U_j] = \mu \gamma_1^i ... \gamma_R^i U_j
$$
Здесь $\mu$ - среднее для базовой ячейки, т.е. для которой $\gamma_r^i = 1, r = 1, ... ,R$.
Для упрощения записи введём
$$
\gamma_1^i ... \gamma_R^i = \gamma_i
$$

Как и ранее
\begin{equation} \label{eq:redef_UV}
V_j = \mu U_j. 
\end{equation}

Тогда модель можно переписать как:
$$
E[Y_{ijt} | V_j] = \gamma_i V_j
$$




Перепишем предположения сделанные в предыдущем разделе для данной модели.

\begin{itemize}
	\item Случайные векторы $(Y_{ijt}, V_j)$ независимы, $j = 1, ... , J$
	\item $V_j$ одинаково распределены со средним $E[V_j] = \mu > 0$, и дисперсией $D[V_j] = \tau^2$, для некоторого $\tau^2$, $j = 1, ... , J$
	\item $\forall j$ все $Y_{ijt}$ при условии $V_j$ независимы со средним $E[Y_{ijt} | V_j] = V_j$
\end{itemize}


Теперь изменим наблюдаемые переменные, чтобы получть модель Бюльмана-Штрауба:
$$
\widetilde{Y}_{ijt} = \frac{Y_{ijt}}{\gamma_i}, \qquad
\widetilde{w}_{ijt} = w_{ijt} \gamma_i^{2 - p}
$$
Заметим, что теперь
$$
E[\widetilde{Y}_{ijt} | V_j] = V_j
$$
а также 
$$
E[D[\widetilde{Y}_{ijt} | V_j]] = \frac{\sigma^2}{\widetilde{w}_{ijt}}
$$

То есть мы свели нашу модель к модели Бюльмана-Штрауба. А к ней можно применить теорему Бюльмана-Штрауба. Тогда мы получим следующее:


\textbf{Утверждение} Достоверная оценка для $V_j$ может быть найдена как

\begin{equation} \label{eq:cred_est_glm}
	\widehat{V}_j = \widetilde{z}_j \overline{\widetilde{Y}}_j + (1 - \widetilde{z}_j) \mu
\end{equation}

где 
\begin{equation} \label{eq:cred_est_glm_params}
\widetilde{w}_j = \sum_{it} w_{ijt} \quad
\overline{\widetilde{Y}}_j = \frac{\sum_{it} \widetilde{w}_{ijt} \widetilde{Y}_{ijt}}{\widetilde{w}_{j}} \quad
\widetilde{z}_j = \frac{\widetilde{w}_j}{\widetilde{w}_j + \sigma^2/\tau^2}
\end{equation}



\subsubsection{Оценка параметров в мультипликативных моделях}

Оценка параметров с учётом видоизменения наблюдаемых переменных почти не отличается от модели Бюльмана-Штрауба.

\begin{equation} \label{eq:mu_est_glm}
	\mu = \frac{\sum_{ijt}\widetilde{w}_{ijt}\widetilde{Y}_{ijt}}{\sum_{ijt}\widetilde{w}_{ijt}}
\end{equation}

$$
\widehat{\sigma}^2_j = \frac{1}{n_j - 1} \sum_{it} \widetilde{w}_{ijt} (\widetilde{Y}_{ijt} - \overline{\widetilde{Y}}_{j})^2 
$$
комбинируя, получаем 
\begin{equation} \label{eq:sigma_est_glm}
\widehat{\sigma}^2 = \frac{\sum_j(n_j - 1) \widehat{\sigma}^2_j}{\sum_j(n_j - 1)} 
\end{equation}


\begin{equation} \label{eq:tau_est_glm}
\widehat{\tau}^2 = \frac{\sum_t \widetilde{w}_{j} (\overline{\widetilde{Y}}_j - \overline{\widetilde{Y}}) ^ 2 - (J - 1) \widehat{\sigma}^2}
{\widetilde{w} - \sum_j \widetilde{w}_j^2 / \widetilde{w}} 
\end{equation}



К сожалению, в данном случае о несмещённости данных оценок говорить можно лишь в том случае, когда все коэффиценты $\gamma_i$ точно известны. На практике, это почти невозможно, коэффициенты находятся по максимальному правдоподобию.

\subsubsection{Backfitting алгоритм}

Как только мы имеем коэффиценты для нашей модели, мы можем оценить $\widehat{U}_j$. А после этого, используя эти оценки для смещений $\widehat{U}_j$, как вектор известных смещений $\xi$, мы можем улучшить модель, заново пересчитав коффициенты. затем опять пересчитать оценки для $\widehat{U}_j$ уже на новых коэффициентах и т.д. Эти несложные рассуждения приводят нас к backfitting алгоритму. Запишем его в итеративном виде.
\begin{enumerate}
	\item[0] Первоначально положим $\widehat{U}_j = 1, \forall j$ 
	\item Используя имеющиеся $\widehat{U}_j$ как вектор известных смещений $\xi$, оцениваем коэффициенты $\gamma_r^i$ обычным для мультипликативной модели образом, описаном в \ref{beta_est}
	\item Пересчитываем $\widehat{U}_j$ согласно формуле \eqref{eq:cred_est_glm}, используя замену \eqref{eq:redef_UV}. Параметры дисперсии оцениваем, по формулам \eqref{eq:tau_est_glm}, \eqref{eq:sigma_est_glm} и если необходимо  \eqref{eq:mu_est_glm} . 
	\item Возвращаемся к шагу 0.
\end{enumerate}
Повторяем шаги 1-4 до сходимости.

Заметим, что использование начального приближения $\widehat{U}_j = 1, \forall j$ на шаге 0, равносильно тому, что данный многоуровневый фактор просто не включается в модель.

\newpage

\section{Модификация backfitting алгоритма}
	В данной работе была использована модификация backfitting алгоритма. А именно, шага 0. Стандартный алгоритм, просто не учитывает влияние данного фактора в начальной точке. Данная модификация предусмаривает использование коэффициентов, полученных при включении данного фактора обычным образом в модель. Детали выбираются в зависимости от конкретной ситуации. Например, при моделировании частоты, как было показано ранее, нельзя оставлять уровни, в которых не было ни одного инцидента. В таких случаях, можно различать лишь несколько уровней с наибольшим числом данных, остальные помещать на отдельный уровень. 
	
	Теперь начальная точка учитывает влияние данного фактора на модель. Данная модификация мотивируется вполне интуитивным соображением, что подбор коэффициентов $V_j$ отличается от подбора коэффициентов $\gamma^i = e^{\beta_i}$ лишь тем, что первые выбираются при фиксированных остальных коэффициентов, а вторые в совокупности с ними. Таким образом модифицированный backfitting алгоритм можно записать итеративно, как:
	
\begin{enumerate}
	\item[0] Первоначально для $\widehat{U}_j$ используем коэффициенты, полученные при включении данного фактора в модель обычным образом. После, все остальные коэффициенты принимают прежнее значение.
	\item Используя имеющиеся $\widehat{U}_j$ как вектор известных смещений $\xi$, оцениваем коэффициенты $\gamma_r^i$ обычным для мультипликативной модели образом, описаном в \ref{beta_est}
	\item Пересчитываем $\widehat{U}_j$ согласно формуле \eqref{eq:cred_est_glm}, используя замену \eqref{eq:redef_UV}. Параметры дисперсии оцениваем, по формулам \eqref{eq:tau_est_glm}, \eqref{eq:sigma_est_glm} и если необходимо  \eqref{eq:mu_est_glm} . 
	\item Возвращаемся к шагу 0.
\end{enumerate}
Повторяем шаги 1-4 до сходимости.
	